"""Pipeline ÁºñÊéíÂô® ‚Äî Ë∑ØÁî±Â±ÇÂîØ‰∏ÄÈúÄË¶ÅË∞ÉÁî®ÁöÑÂÖ•Âè£

ËÅåË¥£Ôºö
1. ÈÖçÁΩÆ LLMÔºàÊ≥®ÂÜåÂà∞ LLMManagerÔºâ
2. ÈÄâÊã©ÁÆ°Á∫øÔºàLLM / OCRÔºâ
3. ÁÆ°ÁêÜÂºÇÊ≠•‰ªªÂä°ÔºàÂàõÂª∫„ÄÅÂèñÊ∂à„ÄÅÊ∏ÖÁêÜÔºâ
4. ‰øùÂ≠òÁøªËØëÁªìÊûú
"""
import asyncio
import logging
import time
from typing import Optional

from backend.app.services.task_manager import get_task_manager, TaskInfo
from backend.app.services.translation_store import get_translation_store
from backend.app.services.translator import TranslationService
from backend.app.services.llm_setup import LLMSetupService
from core.llm.config import FunctionKey
from core.llm.manager import get_llm_manager
from core.ocr.manager import get_ocr_manager
from .base import BasePipeline, PipelineResult, CancellationToken
from .llm_pipeline import LLMPipeline
from .ocr_pipeline import OCRPipeline

logger = logging.getLogger(__name__)


class PipelineOrchestrator:
    """ÁÆ°Á∫øÁºñÊéíÂô®"""

    async def process(
        self,
        file_content: bytes,
        filename: str,
        provider: str,
        model: str,
        api_key: str,
        enable_ocr: bool = False,
        system_prompt: Optional[str] = None,
    ) -> dict:
        """
        ÂÆåÊï¥Â§ÑÁêÜÊµÅÁ®ãÔºöÈÖçÁΩÆ LLM ‚Üí ÈÄâÊã©ÁÆ°Á∫ø ‚Üí ÊâßË°å ‚Üí ‰øùÂ≠òÁªìÊûú ‚Üí ËøîÂõûÂìçÂ∫îÊï∞ÊçÆ„ÄÇ

        Returns:
            dict: ÂèØÁõ¥Êé•‰Ωú‰∏∫ API ÂìçÂ∫îËøîÂõûÁöÑÂ≠óÂÖ∏
        """
        # 1. ÈÖçÁΩÆ LLM
        self._setup_llm(provider, model, api_key)

        # 2. ÂàõÂª∫‰ªªÂä°
        tm = get_task_manager()
        task_info = tm.create_task(filename)
        token = CancellationToken()

        logger.info(f"üìÑ Â§ÑÁêÜ: {filename} | LLM={provider}/{model} | OCR={'ÂºÄ' if enable_ocr else 'ÂÖ≥'} | task={task_info.task_id}")

        try:
            job_start = time.time()

            # 3. ÈÄâÊã©Âπ∂ÊâßË°åÁÆ°Á∫ø
            pipeline = self._select_pipeline(enable_ocr, system_prompt, token)
            pipeline_task = asyncio.create_task(pipeline.execute(file_content, filename))
            task_info.asyncio_tasks = [pipeline_task]

            try:
                result = await pipeline_task
            except asyncio.CancelledError:
                logger.info(f"üõë ‰ªªÂä°Â∑≤ÂèñÊ∂à: {task_info.task_id}")
                return {"error": "cancelled", "task_id": task_info.task_id}

            total_time = time.time() - job_start
            logger.info(f"üéâ ‰ªªÂä°ÂÆåÊàê | ÊÄªËÄóÊó∂ {total_time:.1f}s")

            # 4. ‰øùÂ≠òÁªìÊûú
            entry = await self._save_result(filename, result, provider, model, enable_ocr)

            # 5. ÊûÑÂª∫ÂìçÂ∫î
            translator = await TranslationService.from_manager(FunctionKey.TRANSLATION)
            return self._build_response(task_info, entry, result, translator, model)

        except asyncio.CancelledError:
            return {"error": "cancelled", "task_id": task_info.task_id}
        finally:
            tm.finish_task(task_info.task_id)

    def _setup_llm(self, provider: str, model: str, api_key: str):
        """Á°Æ‰øù translation binding ÂèØÁî® ‚Äî ‰∏çÂÜçÂàõÂª∫‰∏¥Êó∂ profileÔºåËÄåÊòØÂ§çÁî®Áî®Êà∑ÈÖçÁΩÆÁöÑÊ°£Ê°à"""
        manager = get_llm_manager()
        # Â¶ÇÊûúÁî®Êà∑Â∑≤ÁªèÈÄöËøá binding ÈÖçÁΩÆ‰∫Ü translationÔºåÁõ¥Êé•Áî®
        bindings = manager.get_all_bindings()
        bound = bindings.get("translation")
        if bound and manager.get_profile(bound):
            return
        # Âê¶ÂàôÂõûÈÄÄÔºöÁî®ËØ∑Ê±ÇÂèÇÊï∞‰∏¥Êó∂Ê≥®ÂÜåÔºàÂÖºÂÆπÊóßÈÄªËæëÔºâ
        LLMSetupService.ensure_translation_ready(provider, model, api_key)

    def _select_pipeline(
        self,
        enable_ocr: bool,
        system_prompt: Optional[str],
        token: CancellationToken,
    ) -> BasePipeline:
        """Ê†πÊçÆÂèÇÊï∞ÈÄâÊã©ÁÆ°Á∫ø"""
        if enable_ocr:
            ocr_mgr = get_ocr_manager()
            if ocr_mgr.has_binding("ocr"):
                return OCRPipeline(system_prompt=system_prompt, token=token)
            else:
                logger.warning("‚ö†Ô∏è  OCR Â∑≤ÂêØÁî®‰ΩÜÊú™ÁªëÂÆö ProviderÔºåÂõûÈÄÄÂà∞ LLM ÁÆ°Á∫ø")
        return LLMPipeline(system_prompt=system_prompt, token=token)

    async def _save_result(
        self,
        filename: str,
        result: PipelineResult,
        provider: str,
        model: str,
        enable_ocr: bool,
    ) -> dict:
        """‰øùÂ≠òÁøªËØëÁªìÊûúÂà∞ Translation/{id}/ Êñá‰ª∂Â§π"""
        store = get_translation_store()
        profile = result.prompt_profile
        return await store.save(
            filename=filename,
            translated_md=result.translated_md,
            images=result.images,
            ocr_md=result.ocr_md,
            ocr_images=result.ocr_images,
            meta_extra={
                "provider": provider,
                "model": model,
                "enable_ocr": enable_ocr,
                "prompt_profile": {
                    "domain": profile.domain if profile else "",
                    "terminology_count": len(profile.terminology) if profile else 0,
                } if profile else None,
            },
        )

    @staticmethod
    def _build_response(
        task_info: TaskInfo,
        entry: dict,
        result: PipelineResult,
        translator: TranslationService,
        model: str,
    ) -> dict:
        """ÊûÑÂª∫ API ÂìçÂ∫î"""
        profile = result.prompt_profile
        return {
            "task_id": task_info.task_id,
            "translation_id": entry["id"],
            "markdown": result.translated_md,
            "ocr_markdown": result.ocr_md,
            "provider_used": translator.get_provider_name(),
            "model_used": model,
            "prompt_profile": {
                "domain": profile.domain if profile else "",
                "terminology_count": len(profile.terminology) if profile else 0,
                "keep_english": profile.keep_english if profile else [],
                "generated_prompt": profile.translation_prompt if profile else "",
            } if profile else None,
        }
